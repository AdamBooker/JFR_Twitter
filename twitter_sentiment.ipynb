{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates a method for deriving sentiment from social media posts. We are going to use a transformer model named finbert. Finbert was trained on financial data and has three possible outputs: positive, negative, or neutral. This tutorial requires downloading of ~500MB of neural network files. Classification and training are faster when using a graphic card. The code is based on pytorch, which often doesn't play nice with the latest version of CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the imports that we need to run the sentiment analysis. Use \"pip install\" for imports that are not available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import html\n",
    "import tensorflow as tf\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                            pipeline, TrainingArguments, Trainer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function below to pre-parse the tweets. Calling html.unescape removes the transformed characters in html. Add the command print(post) before and after the line that calls html.unescape to get a better idea of how it works. Next, we split the post into words and use regular expressions to modify words that might cause overfitting (such as the ticker of a firm that is discussed a lot, or a user that is always bullish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lite_parser(post):\n",
    "    # remove html chars %xx\n",
    "    post = html.unescape(post)\n",
    "\n",
    "    # look at each word\n",
    "    t_post = post.split()\n",
    "    \n",
    "    # make links, mentions, cashtags, and numbers uniform\n",
    "    t_post = [re.sub('^http.*', '#link',z) for z in t_post]\n",
    "    t_post = [re.sub('^\\@.*', '@mention',z) for z in t_post]\n",
    "    t_post = [re.sub('^\\$.*', '$cashtag', z) for z in t_post]\n",
    "    t_post = [re.sub('^\\d+\\.*\\d+', '#number', z) for z in t_post]\n",
    "\n",
    "    # blunt instrument used to remove things that are not in the set of symbols below\n",
    "    # more can be added here\n",
    "    t_post = [re.sub(\"[^a-zA-Z@#$0-9.,!?']\", ' ', z) for z in t_post]\n",
    "  \n",
    "    #remove blanks\n",
    "    t_post = ' '.join([z.strip() for z in t_post if len(z.strip()) > 0]).strip().split('\\w+')\n",
    "    return ' '.join(t_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose from many different pretrained models on huggingface.co. For this tutorial, the transformer finbert, tuned by ProsusAI (available here: https://huggingface.co/ProsusAI/finbert) is going to be used to classify social media posts as positive, negative, or neutral. Researchers can also build models from scratch using pytorch or tensorflow. Because the model is available from Hugging Face, you can add the relative address of the model (relative to huggingface.co/[ProsusAI/finbert]) and it will download to your computer. We use pipeline wrap our neural network in a manner that makes coding classification neater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert', num_labels = 3)\n",
    "nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file at in/jfr_sample.txt. This file has posts and their associated sentiment. 0 is negative, 1 is positive, and 2 is neutral. Use the xmap disctionary to connect the sample data with the model data. Use classification report along with lists of the predicted and manually classified output values to test how well the model works with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.28      0.40       115\n",
      "           1       0.77      0.20      0.32       115\n",
      "           2       0.39      0.90      0.54       115\n",
      "\n",
      "    accuracy                           0.46       345\n",
      "   macro avg       0.62      0.46      0.42       345\n",
      "weighted avg       0.62      0.46      0.42       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('in/jfr_sample.txt', delimiter='\\t')\n",
    "\n",
    "# use lite_parser on each line of the input file\n",
    "df['texts'] = df['texts'].apply(lite_parser)\n",
    "\n",
    "# classified is a json-like data structure with the results of the classification\n",
    "classified = nlp(df['texts'].to_list())\n",
    "\n",
    "# map sentiment to jfr_sample label\n",
    "xmap = {'neutral': 2,'positive': 1,'negative': 0}\n",
    "y_predict = [xmap[z['label']] for z in classified]\n",
    "y_true = df['label'].to_list()\n",
    "\n",
    "# Get model performance\n",
    "report = classification_report(y_pred=np.array(y_predict),y_true=np.array(y_true))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance could probably be better. The initial model wasn't designed for social media. Instead of creating a new model, we could try to tune this model to work better with our data. A simple way to add words is shown below. Care should be taken to ensure that the embeddings for these words are being trained since they start as random numbers (equivalent to noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['#link', '@mention', '$cashtag', '#number'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the labels in the downloaded model and align numerical categorizations of model and new sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0, 'positive'), (1, 'negative'), (2, 'neutral')])\n"
     ]
    }
   ],
   "source": [
    "# check model labels and align with fine tuning data labels (here, we switch tuning positive and negative)\n",
    "print(model.config.id2label.items())\n",
    "sent_map = {1:0,0:1,2:2}\n",
    "df['label'] = df['label'].map(sent_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment is copied from: https://github.com/yya518/FinBERT/blob/master/finetune.ipynb\n",
    "\n",
    "Below, split the input data into training, validation, and holdout test sets and translate the data to the native pytorch data format.\n",
    "\n",
    "The output notes that the model isn't using the raw text of the posts or the index of dataset, which makes sense because translations of the posts into numerical data is what the model uses (numerical data such as ids that map to a vocabulary dictionary of words that are represented as vectors of numbers and other vectors that help the model interpret context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.71ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 199.97ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.75ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 279\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 45\n",
      " 20%|██        | 9/45 [00:57<03:34,  5.95s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 32\n",
      "                                              \n",
      " 20%|██        | 9/45 [00:59<03:34,  5.95s/it]Saving model checkpoint to models/checkpoint-9\n",
      "Configuration saved in models/checkpoint-9\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0919320583343506, 'eval_accuracy': 0.5483870967741935, 'eval_runtime': 2.1976, 'eval_samples_per_second': 14.106, 'eval_steps_per_second': 0.455, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-9\\pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-9\\tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-9\\special_tokens_map.json\n",
      " 40%|████      | 18/45 [01:57<02:41,  5.99s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 32\n",
      "                                               \n",
      " 40%|████      | 18/45 [01:59<02:41,  5.99s/it]Saving model checkpoint to models/checkpoint-18\n",
      "Configuration saved in models/checkpoint-18\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0437190532684326, 'eval_accuracy': 0.4838709677419355, 'eval_runtime': 2.2428, 'eval_samples_per_second': 13.822, 'eval_steps_per_second': 0.446, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-18\\pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-18\\tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-18\\special_tokens_map.json\n",
      " 60%|██████    | 27/45 [02:57<01:48,  6.00s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 32\n",
      "                                               \n",
      " 60%|██████    | 27/45 [03:00<01:48,  6.00s/it]Saving model checkpoint to models/checkpoint-27\n",
      "Configuration saved in models/checkpoint-27\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1979701519012451, 'eval_accuracy': 0.45161290322580644, 'eval_runtime': 2.2303, 'eval_samples_per_second': 13.9, 'eval_steps_per_second': 0.448, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-27\\pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-27\\tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-27\\special_tokens_map.json\n",
      " 80%|████████  | 36/45 [03:57<00:53,  5.99s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 32\n",
      "                                               \n",
      " 80%|████████  | 36/45 [04:00<00:53,  5.99s/it]Saving model checkpoint to models/checkpoint-36\n",
      "Configuration saved in models/checkpoint-36\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1972733736038208, 'eval_accuracy': 0.4838709677419355, 'eval_runtime': 2.1965, 'eval_samples_per_second': 14.113, 'eval_steps_per_second': 0.455, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-36\\pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-36\\tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-36\\special_tokens_map.json\n",
      "100%|██████████| 45/45 [04:57<00:00,  6.00s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, texts. If __index_level_0__, texts are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 32\n",
      "                                               \n",
      "100%|██████████| 45/45 [04:59<00:00,  6.00s/it]Saving model checkpoint to models/checkpoint-45\n",
      "Configuration saved in models/checkpoint-45\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1989370584487915, 'eval_accuracy': 0.4838709677419355, 'eval_runtime': 2.2012, 'eval_samples_per_second': 14.083, 'eval_steps_per_second': 0.454, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/checkpoint-45\\pytorch_model.bin\n",
      "tokenizer config file saved in models/checkpoint-45\\tokenizer_config.json\n",
      "Special tokens file saved in models/checkpoint-45\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/checkpoint-9 (score: 0.5483870967741935).\n",
      "100%|██████████| 45/45 [05:01<00:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 301.6007, 'train_samples_per_second': 4.625, 'train_steps_per_second': 0.149, 'train_loss': 0.6538941701253255, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=0.6538941701253255, metrics={'train_runtime': 301.6007, 'train_samples_per_second': 4.625, 'train_steps_per_second': 0.149, 'train_loss': 0.6538941701253255, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# split data\n",
    "df_train, df_test, = train_test_split(df, stratify=df['label'], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train['label'],test_size=0.1, random_state=42)\n",
    "\n",
    "# translate data\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_test = dataset_test.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length' , max_length=128), batched=True)\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# train/save model\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy' : accuracy_score(predictions, labels)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = 'models/',\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=args,                  # training arguments, defined above\n",
    "        train_dataset=dataset_train,         # training dataset\n",
    "        eval_dataset=dataset_val,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted average f1-score went from 0.42 to 0.83 in my model. However, in this finetuned model we are blending data that we used to train the model with holdout data. Add the command 'df_test.to_csv('out/holdout.csv')' to save holdout data in the prior step. Load that data in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file models/checkpoint-45/vocab.txt\n",
      "loading file models/checkpoint-45/tokenizer.json\n",
      "loading file models/checkpoint-45/added_tokens.json\n",
      "loading file models/checkpoint-45/special_tokens_map.json\n",
      "loading file models/checkpoint-45/tokenizer_config.json\n",
      "loading configuration file models/checkpoint-45/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models/checkpoint-45/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30526\n",
      "}\n",
      "\n",
      "loading weights file models/checkpoint-45/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/checkpoint-45/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       115\n",
      "           1       0.81      0.80      0.80       115\n",
      "           2       0.78      0.85      0.82       115\n",
      "\n",
      "    accuracy                           0.83       345\n",
      "   macro avg       0.83      0.83      0.83       345\n",
      "weighted avg       0.83      0.83      0.83       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('models/checkpoint-45/')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('models/checkpoint-45/', num_labels = 3)\n",
    "nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "df = pd.read_csv('in/jfr_sample.txt', delimiter='\\t')\n",
    "df['texts'] = df['texts'].apply(lite_parser)\n",
    "\n",
    "x = nlp(df['texts'].to_list())\n",
    "\n",
    "xmap = {'neutral': 2,'positive': 1,'negative': 0}\n",
    "\n",
    "y_predict = [xmap[z['label']] for z in x]\n",
    "y_true = df['label'].to_list()\n",
    "\n",
    "report = classification_report(y_pred=np.array(y_predict),y_true=np.array(y_true))\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
